{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Play Generator RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbvqSlC-DBVj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bcbed611-5875-445f-b5e4-07d2e2467c0b"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hido3IAvDPzK",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "17cec8c0-14ea-4651-f2ae-c812b64f2f57"
      },
      "source": [
        "#import data\n",
        "#path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "#importing my own data\n",
        "\n",
        "from google.colab import files\n",
        "path_to_file = list(files.upload().keys())[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e350ef75-5749-46e0-b347-81206f11e7a7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e350ef75-5749-46e0-b347-81206f11e7a7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving dave_chappelle_transcripts.txt to dave_chappelle_transcripts.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFF8gteaLvqs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3eac624-7b5d-48c1-d839-b3d5024a71fd"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 340030 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py1OXuCEMj2M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74a7c11d-1981-4a44-d958-35a2aa0fb825"
      },
      "source": [
        "#data preprocessing\n",
        "#encode the text into integers (characters)\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)\n",
        "\n",
        "# function converting integer back to text\n",
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sometimes, th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmoHL1rOSU4L"
      },
      "source": [
        "# Create training examples\n",
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets ; create a stream of characters\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "# use batch method to turn stream of characters into batches\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "#splitting the data\n",
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljt2LCVCWo_3"
      },
      "source": [
        "# make training batches\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upQaL9l2u8A3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "ac10179b-fbe9-42bc-f779-4df99c847f06"
      },
      "source": [
        "# build model\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           23296     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 91)            93275     \n",
            "=================================================================\n",
            "Total params: 5,363,547\n",
            "Trainable params: 5,363,547\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb065oahvgeC"
      },
      "source": [
        "# creating a loss function\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4NLF672Fw3K"
      },
      "source": [
        "#compile the model\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ht1ICpuF_Iq"
      },
      "source": [
        "#create checkpoints: saves checkpoinst as it trains. This will allow us to load our model from a checkpoint and continue training it.\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89NvddDQGNxu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dbec9b17-e94b-4d1a-c528-e954a1c17280"
      },
      "source": [
        "#training the model\n",
        "history = model.fit(data, epochs=100, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "52/52 [==============================] - 4s 72ms/step - loss: 3.1922\n",
            "Epoch 2/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 2.4972\n",
            "Epoch 3/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 2.1769\n",
            "Epoch 4/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 1.9592\n",
            "Epoch 5/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 1.8043\n",
            "Epoch 6/100\n",
            "52/52 [==============================] - 4s 75ms/step - loss: 1.6869\n",
            "Epoch 7/100\n",
            "52/52 [==============================] - 4s 76ms/step - loss: 1.5955\n",
            "Epoch 8/100\n",
            "52/52 [==============================] - 4s 75ms/step - loss: 1.5170\n",
            "Epoch 9/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 1.4508\n",
            "Epoch 10/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 1.3954\n",
            "Epoch 11/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 1.3428\n",
            "Epoch 12/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 1.2979\n",
            "Epoch 13/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 1.2530\n",
            "Epoch 14/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 1.2092\n",
            "Epoch 15/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 1.1681\n",
            "Epoch 16/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 1.1256\n",
            "Epoch 17/100\n",
            "52/52 [==============================] - 4s 72ms/step - loss: 1.0831\n",
            "Epoch 18/100\n",
            "52/52 [==============================] - 4s 72ms/step - loss: 1.0397\n",
            "Epoch 19/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.9960\n",
            "Epoch 20/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.9524\n",
            "Epoch 21/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.9030\n",
            "Epoch 22/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.8547\n",
            "Epoch 23/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.8060\n",
            "Epoch 24/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.7580\n",
            "Epoch 25/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.7126\n",
            "Epoch 26/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.6653\n",
            "Epoch 27/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.6196\n",
            "Epoch 28/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.5806\n",
            "Epoch 29/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.5405\n",
            "Epoch 30/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.5058\n",
            "Epoch 31/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.4716\n",
            "Epoch 32/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.4439\n",
            "Epoch 33/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.4159\n",
            "Epoch 34/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.3949\n",
            "Epoch 35/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.3759\n",
            "Epoch 36/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.3595\n",
            "Epoch 37/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.3434\n",
            "Epoch 38/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.3328\n",
            "Epoch 39/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.3189\n",
            "Epoch 40/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.3107\n",
            "Epoch 41/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2994\n",
            "Epoch 42/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2935\n",
            "Epoch 43/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.2858\n",
            "Epoch 44/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2802\n",
            "Epoch 45/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2736\n",
            "Epoch 46/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2682\n",
            "Epoch 47/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2642\n",
            "Epoch 48/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2591\n",
            "Epoch 49/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.2542\n",
            "Epoch 50/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2509\n",
            "Epoch 51/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.2456\n",
            "Epoch 52/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2439\n",
            "Epoch 53/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2418\n",
            "Epoch 54/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.2386\n",
            "Epoch 55/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2368\n",
            "Epoch 56/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2329\n",
            "Epoch 57/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2316\n",
            "Epoch 58/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2287\n",
            "Epoch 59/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2266\n",
            "Epoch 60/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2247\n",
            "Epoch 61/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2220\n",
            "Epoch 62/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2197\n",
            "Epoch 63/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2202\n",
            "Epoch 64/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2157\n",
            "Epoch 65/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.2164\n",
            "Epoch 66/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2131\n",
            "Epoch 67/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2121\n",
            "Epoch 68/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2121\n",
            "Epoch 69/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.2102\n",
            "Epoch 70/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2070\n",
            "Epoch 71/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2071\n",
            "Epoch 72/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2065\n",
            "Epoch 73/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.2049\n",
            "Epoch 74/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2030\n",
            "Epoch 75/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2034\n",
            "Epoch 76/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2024\n",
            "Epoch 77/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.2005\n",
            "Epoch 78/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1999\n",
            "Epoch 79/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1986\n",
            "Epoch 80/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1961\n",
            "Epoch 81/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1966\n",
            "Epoch 82/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1948\n",
            "Epoch 83/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1929\n",
            "Epoch 84/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1939\n",
            "Epoch 85/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1919\n",
            "Epoch 86/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1904\n",
            "Epoch 87/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1909\n",
            "Epoch 88/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1924\n",
            "Epoch 89/100\n",
            "52/52 [==============================] - 4s 74ms/step - loss: 0.1903\n",
            "Epoch 90/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1890\n",
            "Epoch 91/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1881\n",
            "Epoch 92/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1872\n",
            "Epoch 93/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1858\n",
            "Epoch 94/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1855\n",
            "Epoch 95/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1850\n",
            "Epoch 96/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1844\n",
            "Epoch 97/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1849\n",
            "Epoch 98/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1828\n",
            "Epoch 99/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1829\n",
            "Epoch 100/100\n",
            "52/52 [==============================] - 4s 73ms/step - loss: 0.1808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppdFrhY6G-Ip"
      },
      "source": [
        "#load and rebuild model to pass in input samples of batch size 1\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)\n",
        "\n",
        "#find the latest checkpoint that stores the models weights\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "#[1 = dimension 1, None = don't know what size of the input it will be]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grPiHFpyIHcr"
      },
      "source": [
        "#text generating function by tensorflow\n",
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 8000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0) # places input_eval list into a nested list [] --> [[]], as model expects this as input\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.?\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0) # removes outer [], [[]] --> []\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlUot-K_IcP4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "a023cc8d-7ecc-4cd9-ce48-6f3f6b2e69be"
      },
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting string: thank\n",
            "thank I’m gonna do in this day and age?” You know what I mean? Cause I was doing some “My This will get our grass a decision?” And when the mom came in and she didn’t say it offended her. What she said is, she said, “I am uncomfortable with that word?” And yet, mine. Shit was my shoulder right now. ”And this guy’s balls are as smells like weed. He was speeding. This man isn the beginning of the back of the bus after you again, Dave Chappelle.” And I looked at my friend like they were very dravorch. Who if he’s drink. They want drink. They don’t want all them vitamins, man. There was a big dance coming? I said “has a white dude walking across my property, ent, the racism down there is just here. He also men go through just for the rent!” [laughing] “Hello? Dan’t want to talk about it. The tape. But then Kevin said, “Fuck it, I quit.” And then he went down like pancake butter. He was higher when stay morthfull decisions all the time. You don’t wanna get pissed on, get out of the way. It’s not even due yet, it’s the bar seit on it. ”Caloing on my own problems, that’s a good guy. Like, I don’t know, 3 tashe should’ve been the top story. She chewed through the nature of my pro-choice? Never been throng from Louis. That was like… I don’t like them. Here you go, buddick out and this kid goes out I said, “Fuck, I’m busted a story, that has nothing… and then suddenly, a boxer rises from amongst things becau out here, nigga. I got kids to feed.” I was like, “God DAMN.”\n",
            "\n",
            "Sad. Now just at that very moment, on the first time, ge a big did, which is not the same car ing now. You have to. It’s not a racial thing. It’s about like white dudes do about would be coming out of my eyes and anus, so I got concerned, but I’m okay.” What did does do appear. Fucking niggers him a sex tape. Have you ever heard of suck of these now.” That shit was drink, but he had chicken grease all over his face, so I stiffed on him, like, “Yo, yo, yo, yo, yo, my man. Wethore or something? You sure that’s what you was doing Chappelle’s Show… There’s teomber with them don’t know things gy Hollywood me. More than any of them had ever seen. Iceberg took a little bit of the money in Dallas? This is Michael Jackson my anuthing] The only reason I bring him up, but I was 12 years old. Can you imagine such a thing? Kevin Hart, let’s go to the ban not shit is sometimes “Y.” [laughing] [laughs] And you know the tough parant, even know where the fuck I’m from. They were put on the botton like, “I’m sorry talk like that. I got her and shut for from television. Every black people have go over cresident, trying to revives. And we go, “Man, what is not a joke. It’s a truth is, I don’t. But if you have a reason to hit if my family. And you can imagine, when we read that shit, we was like, “Ooh! You lying-ass, bitch.” Was furious. And this is very good are, ’cause I’ve had no idea. He was a n’t gonna tell. I’m a people look at the cent. I had n chacker $12 mill, a man showed you that bigga to buy my sons some socks at Foot Locker. I go to me. But I read it. I’d already opened it, so I just read the whole never be dure for the teethat is my experience. If I’m started t. I’m rich, beyotch! [car horn honks]\n",
            "\n",
            "[“Kill went? Of course you gon’t remember Thriller. What the fuck he want to meet Michael Jackson for? Honestly. I remember They are the old of them had nothing with the distrest imself was armed, fired back, killing four people that had nothing ret more boy over his fashe part.” [laughing] “I buy one of them had ever seen. Iceberg took a little bit of the menth?” ”Well, what do you mean? You should rollegrat your parents I’m not sure I can bean the way I did, whycause… of all the major cities in America, somehow, people get alogy of me a bettin’ man, I’m gonna put my money on “He probably did that shit.” I’m pretty sure he did that heady s look good.”Hot discrepay? [laughing] The fuck has been going on out here? [laughing] It’s terrible. I know, it’s terrible. I’m not a hard thing to be a comedian. She was. Remember a nice place. Look at this brooking like, ”Yeah, I can’t stop trouble. And just at that money in my backpack, I jumped on the subway He pause up in the middle school. It was weird. He didn’t even turn his radio down. Isn’t that weird, a little bit? I mean you get pussy in my badds and shit like that. [more too much shit looks disguptes. Now you see what it is. But at the time I didn’t think there was anything racial about it. I was like my wife, I said, “Don’t worry about that shit in her face. [laughing] And nobody else’s dick ought my own eyes, I’ve seen a gathering… of 1,500 Natifooplened, DCause I was expendience cherry. He used to own the Clinton again. His name was Et terrorists do. She sadn that Corse I don’t wanna be the first one. I mean the second or the measles do I can tell him.” And I said, “Hey, man, I fuck feet.” He said, “What?!” Oh, this is not a joke, ladies and gentlemen. I feel bad saying it. I’m not getting a ck jirts were highton was gonna be our next president. You just watched that facoul Mositraid of them. You know what I mean. You’re a white lady. You ever been pulled over before. Fear for that shit. It’s a good dude. And then she doesnd shit. The whole stoved years later, the difference. But this T\n",
            "It was tion is: ”Hey, buddy, hey. ”There you go, buddy, year. Yeah, uh, look, there’s a good 36 years because I was quote “raciat.” He said, “Calm down, bitch. We didn’t do anything. What do you mean?” “Huh? So, note active carding here, then beat the 20% of the audience. And unfortunt? [audience laughing] And everyten to me, you see what just come up with I’m fucking from 40 years before Bill Cosby’s first rape. It’s a very old banana, bitch. He’s still, ma’a amouse been there at least a minute!”\n",
            "\n",
            "But that’s what I want to know, under the stove? You find semen like a chorle go good in the first round. I was ready to feel bad about anything I say up here. And I went to real talk, lady. Stop ban that I can just look in her eye.” I said, “What’s going on in the house and watch TV? Like, I want to talk about it. Maybe they pick up on the road. Some white dude in booty music me, bitches. That’s not even they were no matter. He is. And I was like, “Hey, you know what? Don’t. That didn’t feel bad voting for her, but it didn’t feel about it. This as impossible to shoot luck your dick for life do I see John Edwards, and I said, “Stop making that face. That’s offensive.” -“What?” “This is how I feel for my had to rap you orom? It’s expendernick. I was talking about it a fucking next do it? I they went on television. That’s fucked her a big name and right? You ever been on a plane… I get scared to fly. I do it all the time. I’ll be s the time. He was 14-years-old and was from. Because this is Atlick-pressing room… is, ud, “Oh, that’s Brother would never done trying to get me kid getson. That was my own eyes.” I was like, “Niggas, please, save me the semantics. Just trust me. [audience cheering] That’s right. And after this shit… it’s time to e African-Americans crisis face of the media for coning af an her and your phone, nigga. Get that shit out of here. I’m… I’m in the middle of s Jenna do their for it.” But then I thought, it hurt my ghe thing. It could passy I’ve ever been to the White House. It looks like a fucking nightmare.\n",
            "That would be like if you were having sex with a woman and, for some reason– Ain’t no gonna try to take everything away from you, and I don’t care when I find out yetred, they didn’t get mean faces. They got past me. That’s the Obs. It’s easily for me, that motherfucker… took impeccable notes. He took everybody in the eper.” I drove at a twe calling the WNBA white people hat happened to the brother in Dallas? They just rubber with their four stuffing. If you guys know I’m gonna party ♪\n",
            "♪ Like it’s 1999 ♪\n",
            "\n",
            "I knew a nigga in high school that was an urban genius. This man with Charlie Murphy.\n",
            "\n",
            "Tell a stary come for about that nigga Bill Cosby a joke.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyWwbtbEJvUa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}